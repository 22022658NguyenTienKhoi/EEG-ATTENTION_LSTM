{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "c24145bc-fb93-488b-9858-3b1b6bae82d6",
    "_uuid": "c726ce06-48e2-4c51-99e2-99bc45a7902a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-26T02:27:36.790941Z",
     "iopub.status.busy": "2025-05-26T02:27:36.790289Z",
     "iopub.status.idle": "2025-05-26T02:27:39.631411Z",
     "shell.execute_reply": "2025-05-26T02:27:39.630754Z",
     "shell.execute_reply.started": "2025-05-26T02:27:36.790912Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.signal import butter, filtfilt # Keep original filtfilt for the improved func\n",
    "# Removed resample as it wasn't used, can be added back if needed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import skew, kurtosis\n",
    "# Removed pywt as it wasn't used, can be added back if needed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Suppress specific MNE warnings if desired, adjust as needed\n",
    "warnings.filterwarnings(\"ignore\", message=\".*pick_types.*legacy function.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*maximum epoch persistence is not supported.*\") # Common non-critical warning\n",
    "mne.set_log_level('WARNING') # Reduce MNE verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:27:39.632794Z",
     "iopub.status.busy": "2025-05-26T02:27:39.632439Z",
     "iopub.status.idle": "2025-05-26T02:27:39.640150Z",
     "shell.execute_reply": "2025-05-26T02:27:39.639368Z",
     "shell.execute_reply.started": "2025-05-26T02:27:39.632776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def bandpass_filter_improved(data, lowcut=12.0, highcut=35.0, fs=250, order=5):\n",
    "    '''Apply a bandpass filter to the EEG data efficiently.\n",
    "    Args:\n",
    "        data: ndarray of shape (num_samples, num_channels, num_timesteps)\n",
    "              or (num_channels, num_timesteps)\n",
    "        lowcut: float, lower frequency bound\n",
    "        highcut: float, upper frequency bound\n",
    "        fs: int, sampling frequency\n",
    "        order: int, filter order\n",
    "    Returns:\n",
    "        filtered_data: ndarray of the same shape as input\n",
    "    '''\n",
    "    nyquist = 0.5 * 250\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "\n",
    "    # Input validation for frequencies\n",
    "    low = max(0, low) # Ensure low is not negative\n",
    "    high = min(1.0 - 1e-6 , high) # Ensure high is less than 1 (nyquist limit)\n",
    "    \n",
    "    if low >= high:\n",
    "        print(f\"Warning: lowcut({low}Hz) >= highcut({high}Hz). Returning original data.\")\n",
    "        return data\n",
    "\n",
    "    if high >= 1.0 and low <= 0.0: # Effectively no filtering\n",
    "        print(\"Warning: Filter range covers full spectrum. Returning original data.\")\n",
    "        return data\n",
    "    elif low <= 0.0: # Lowpass filter\n",
    "        # print(\"Applying Lowpass Filter\")\n",
    "        b, a = butter(order, high, btype='low')\n",
    "    elif high >= 1.0: # Highpass filter\n",
    "        # print(\"Applying Highpass Filter\")\n",
    "        b, a = butter(order, low, btype='high')\n",
    "    else: # Bandpass filter\n",
    "        # print(\"Applying Bandpass Filter\")\n",
    "        b, a = butter(order, [low, high], btype='band')\n",
    "\n",
    "    # Apply the filter along the last axis (time dimension)\n",
    "    try:\n",
    "        filtered_data = filtfilt(b, a, data, axis=-1, padlen=min(150, data.shape[-1]-1)) # Add padlen\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during filtering: {e}. Data shape: {data.shape}. Filter params: b={b}, a={a}. Returning original data.\")\n",
    "        # Handle short signals that might cause issues with filtfilt's padding\n",
    "        if \"padlen must be less than\" in str(e):\n",
    "             print(\"Signal too short for default padding. Trying minimal padding.\")\n",
    "             try:\n",
    "                 filtered_data = filtfilt(b, a, data, axis=-1, padlen=1)\n",
    "             except Exception as e_inner:\n",
    "                 print(f\"Minimal padding failed: {e_inner}. Returning original data.\")\n",
    "                 return data\n",
    "        else:\n",
    "             return data\n",
    "\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:27:39.641058Z",
     "iopub.status.busy": "2025-05-26T02:27:39.640856Z",
     "iopub.status.idle": "2025-05-26T02:27:39.658846Z",
     "shell.execute_reply": "2025-05-26T02:27:39.658212Z",
     "shell.execute_reply.started": "2025-05-26T02:27:39.641041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_eeg_channelwise(data):\n",
    "    '''\n",
    "    Normalize EEG data using z-score normalization per channel across time.\n",
    "    Handles potential flat channels (std=0).\n",
    "    Args:\n",
    "        data: ndarray of shape (num_samples, num_channels, num_timesteps)\n",
    "              or (num_channels, num_timesteps)\n",
    "    Returns:\n",
    "        data_normalized: ndarray of the same shape as input\n",
    "    '''\n",
    "    # Calculate mean and std along the time axis (last axis)\n",
    "    mean = np.mean(data, axis=-1, keepdims=True)\n",
    "    std = np.std(data, axis=-1, keepdims=True)\n",
    "\n",
    "    # Prevent division by zero for channels with no variance\n",
    "    std[std == 0] = 1e-6  # Replace 0 std with a very small number\n",
    "\n",
    "    return (data - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:27:39.660553Z",
     "iopub.status.busy": "2025-05-26T02:27:39.660285Z",
     "iopub.status.idle": "2025-05-26T02:27:39.674002Z",
     "shell.execute_reply": "2025-05-26T02:27:39.673317Z",
     "shell.execute_reply.started": "2025-05-26T02:27:39.660534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def segment_eeg_improved(data, segment_length):\n",
    "    '''\n",
    "    Segment EEG data into fixed-length non-overlapping segments efficiently.\n",
    "    Args:\n",
    "        data: ndarray of shape (num_samples, num_channels, num_timesteps)\n",
    "        segment_length: int, length of each segment in samples\n",
    "    Returns:\n",
    "        segmented_data: ndarray of shape (num_total_segments, num_channels, segment_length)\n",
    "                       Returns None if input data is empty or segment_length is invalid/too large.\n",
    "    '''\n",
    "    if data is None or data.size == 0 or segment_length <= 0:\n",
    "        #print(\"Warning: Empty data or invalid segment length provided to segment_eeg.\")\n",
    "        return None\n",
    "\n",
    "    num_samples, num_channels, num_timesteps = data.shape\n",
    "\n",
    "    if segment_length > num_timesteps:\n",
    "        #print(f\"Warning: Segment length ({segment_length}) > total timesteps ({num_timesteps}). Cannot segment.\")\n",
    "        return None # Or return an empty array: np.empty((0, num_channels, segment_length))\n",
    "\n",
    "    # Calculate number of full segments per sample\n",
    "    num_segments_per_sample = num_timesteps // segment_length\n",
    "\n",
    "    if num_segments_per_sample == 0:\n",
    "         #print(f\"Warning: No full segments possible with length {segment_length} for timesteps {num_timesteps}.\")\n",
    "         return np.empty((0, num_channels, segment_length))\n",
    "\n",
    "    # Truncate data to include only full segments\n",
    "    total_timesteps_used = num_segments_per_sample * segment_length\n",
    "    truncated_data = data[:, :, :total_timesteps_used]\n",
    "\n",
    "    # Reshape to separate segments\n",
    "    # Shape becomes (num_samples, num_channels, num_segments_per_sample, segment_length)\n",
    "    try:\n",
    "        segmented_data = truncated_data.reshape(num_samples, num_channels, num_segments_per_sample, segment_length)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error reshaping data during segmentation: {e}\")\n",
    "        print(f\"Input shape: {data.shape}, Truncated shape: {truncated_data.shape}\")\n",
    "        print(f\"Target reshape: {(num_samples, num_channels, num_segments_per_sample, segment_length)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # Transpose and reshape to desired output: (num_total_segments, num_channels, segment_length)\n",
    "    # Transpose: (num_samples, num_segments_per_sample, num_channels, segment_length)\n",
    "    segmented_data = segmented_data.transpose(0, 2, 1, 3)\n",
    "    # Reshape: (num_samples * num_segments_per_sample, num_channels, segment_length)\n",
    "    final_shape = (num_samples * num_segments_per_sample, num_channels, segment_length)\n",
    "    segmented_data = segmented_data.reshape(final_shape)\n",
    "\n",
    "    return segmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:27:39.674787Z",
     "iopub.status.busy": "2025-05-26T02:27:39.674596Z",
     "iopub.status.idle": "2025-05-26T02:27:39.695622Z",
     "shell.execute_reply": "2025-05-26T02:27:39.694822Z",
     "shell.execute_reply.started": "2025-05-26T02:27:39.674772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def processing_data_corrected(X, segment_length, fs): # Added segment_length and fs params\n",
    "    \"\"\" Processes a single file's data (assumed shape [1, channels, timesteps]) \"\"\"\n",
    "    if X is None or X.size == 0:\n",
    "        return None\n",
    "\n",
    "    # 1. Filter data\n",
    "    # Use the improved, vectorized filter function\n",
    "    filtered_data = bandpass_filter_improved(X, fs=fs) # Pass fs\n",
    "\n",
    "    # 2. Normalize data\n",
    "    # Use the improved, vectorized normalizer (channel-wise within the file)\n",
    "    normalized_data = normalize_eeg_channelwise(filtered_data)\n",
    "    # 3. Segment data\n",
    "    # Use the improved, vectorized segmenter\n",
    "    segmented_data = segment_eeg_improved(normalized_data, segment_length=segment_length)\n",
    "\n",
    "    # Return ALL segments generated for this file\n",
    "    return segmented_data\n",
    "\n",
    "\n",
    "def mapping_data_path(input_paths, labeled_ids_set): # Use set for efficiency\n",
    "    \"\"\" Filters input paths to include only those whose participant ID is in the labeled set. \"\"\"\n",
    "    input_paths_filtered = []\n",
    "    for input_path in input_paths:\n",
    "        try:\n",
    "            basename = os.path.basename(input_path)\n",
    "            # Assuming format like 'sub-CCXXXXXX_...' - adjust if different\n",
    "            parts = basename.split('_')\n",
    "            if len(parts) > 1 and parts[0].startswith('sub-'):\n",
    "                 participant_id = parts[0] # e.g., 'sub-CC110033'\n",
    "                 if participant_id in labeled_ids_set:\n",
    "                    input_paths_filtered.append(input_path)\n",
    "            else:\n",
    "                 print(f\"Warning: Could not parse participant ID from filename: {basename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing filename {input_path}: {e}\")\n",
    "    return input_paths_filtered\n",
    "\n",
    "\n",
    "def load_data_corrected(data_parents_path, segment_length_seconds=4): # Parameter for segment length\n",
    "    \"\"\" Loads, processes (all segments), and combines data and labels correctly. \"\"\"\n",
    "    print(f\"Starting data loading from: {data_parents_path}\")\n",
    "    print(f\"Using segment length: {segment_length_seconds} seconds\")\n",
    "\n",
    "    input_paths = sorted([os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(data_parents_path)) for f in fn if f.endswith('.fif')])\n",
    "    label_path = os.path.join(data_parents_path, 'filtered_subjects_with_age.tsv') # Use os.path.join\n",
    "\n",
    "    if not os.path.exists(label_path):\n",
    "        print(f\"Error: Label file not found at {label_path}\")\n",
    "        return None, None\n",
    "    if not input_paths:\n",
    "        print(f\"Error: No .fif files found in {data_parents_path}\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        label_df = pd.read_csv(label_path, sep='\\\\t')\n",
    "        # Ensure column names are correct (they might have spaces if copied/pasted)\n",
    "        label_df.columns = [col.strip() for col in label_df.columns]\n",
    "        print(f\"Label file loaded. Columns: {label_df.columns.tolist()}\")\n",
    "        if 'participant_id' not in label_df.columns or 'age' not in label_df.columns:\n",
    "             print(\"Error: Label file must contain 'participant_id' and 'age' columns.\")\n",
    "             return None, None\n",
    "\n",
    "        # Create a mapping from participant_id to age for easier lookup\n",
    "        label_map = label_df.set_index('participant_id')['age'].to_dict()\n",
    "        labeled_ids_set = set(label_df['participant_id'].values) # Use set for faster lookup\n",
    "        print(f\"Found {len(labeled_ids_set)} unique participant IDs with labels.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading or processing label file {label_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    input_paths_filtered = mapping_data_path(input_paths, labeled_ids_set)\n",
    "    print(f\"Found {len(input_paths_filtered)} files matching labeled participants.\")\n",
    "\n",
    "    if not input_paths_filtered:\n",
    "        print(\"Error: No files found that match the participant IDs in the label file.\")\n",
    "        return None, None\n",
    "\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    fs = None # To store sampling frequency (should be consistent)\n",
    "    segment_length_samples = None # Will be set after loading first file\n",
    "    n_channels = None # To store number of channels (should be consistent)\n",
    "\n",
    "    print(f\"Processing {len(input_paths_filtered)} files...\")\n",
    "    for i, input_path in enumerate(tqdm(input_paths_filtered, desc=\"Loading Files\")):\n",
    "        # print(f\"  File {i+1}/{len(input_paths_filtered)}: {os.path.basename(input_path)}\")\n",
    "        try:\n",
    "            raw = mne.io.read_raw_fif(input_path, preload=True) # preload=True needed for get_data\n",
    "\n",
    "            # --- Determine fs, segment length in samples, and n_channels from first file ---\n",
    "            if fs is None:\n",
    "                fs = raw.info['sfreq']\n",
    "                segment_length_samples = int(fs * segment_length_seconds)\n",
    "                print(f\"    Detected fs={fs:.2f} Hz. Using segment length={segment_length_samples} samples ({segment_length_seconds}s).\")\n",
    "                if segment_length_samples == 0:\n",
    "                     print(\"Error: Calculated segment length in samples is 0. Check fs and segment_length_seconds.\")\n",
    "                     return None, None\n",
    "            elif abs(fs - raw.info['sfreq']) > 1000 : # Check for consistency\n",
    "                 print(f\"    WARNING: Inconsistent sampling frequency! Expected ~{fs:.2f}, got {raw.info['sfreq']:.2f}. Skipping file: {os.path.basename(input_path)}\")\n",
    "                 continue\n",
    "\n",
    "            # --- Select Channels ---\n",
    "            # !!! CRITICAL: Changed misc=True to eeg=True !!!\n",
    "            # Adjust if your channels are truly 'misc', but usually EEG channels are 'eeg'\n",
    "            # Exclude 'bads' and common non-EEG channels like EOG, ECG if present\n",
    "            try:\n",
    "                 picks = mne.pick_types(raw.info, misc=True, meg=False, stim=False, eog=False, ecg=False, exclude='bads')\n",
    "                 if len(picks) == 0:\n",
    "                      # Fallback or alternative? Maybe check 'misc' if 'eeg' fails?\n",
    "                      print(f\"    WARNING: No 'eeg' channels found in {os.path.basename(input_path)}. Trying 'misc'.\")\n",
    "                      picks = mne.pick_types(raw.info, misc=True, exclude='bads')\n",
    "                      if len(picks) == 0:\n",
    "                           print(f\"    WARNING: No 'eeg' or 'misc' channels found. Skipping file.\")\n",
    "                           continue\n",
    "\n",
    "                 channels_to_process = raw.copy().pick(picks=picks)\n",
    "                 current_n_channels = len(channels_to_process.ch_names)\n",
    "                 #print(f\"    Selected {current_n_channels} channels.\")\n",
    "\n",
    "            except Exception as pick_error:\n",
    "                 print(f\"    Error picking channels for {os.path.basename(input_path)}: {pick_error}. Skipping file.\")\n",
    "                 continue\n",
    "\n",
    "\n",
    "            if n_channels is None:\n",
    "                n_channels = current_n_channels\n",
    "                print(f\"    Setting expected number of channels to {n_channels}.\")\n",
    "            elif n_channels != current_n_channels:\n",
    "                 print(f\"    WARNING: Inconsistent number of channels! Expected {n_channels}, got {current_n_channels}. Skipping file: {os.path.basename(input_path)}\")\n",
    "                 continue\n",
    "\n",
    "\n",
    "            # --- Get Data ---\n",
    "            data = channels_to_process.get_data() # Shape: (num_channels, num_timesteps)\n",
    "            if data.shape[1] < segment_length_samples:\n",
    "                 print(f\"    WARNING: File duration ({data.shape[1]/fs:.2f}s) is shorter than segment length ({segment_length_seconds}s). Skipping file: {os.path.basename(input_path)}\")\n",
    "                 continue\n",
    "\n",
    "            # Add a sample dimension for processing functions expecting (samples, channels, time)\n",
    "            data_batch = np.array([data]) # Shape: (1, num_channels, num_timesteps)\n",
    "\n",
    "            # --- Process data - this now returns ALL segments for this file ---\n",
    "            # Shape: (num_segments_in_file, num_channels, segment_length_samples)\n",
    "            data_processed_segments = processing_data_corrected(data_batch, segment_length=segment_length_samples, fs=10)\n",
    "\n",
    "\n",
    "            # --- Accumulate Data and Labels ---\n",
    "            if data_processed_segments is not None and data_processed_segments.shape[0] > 0:\n",
    "                num_segments_in_file = data_processed_segments.shape[0]\n",
    "                # print(f\"    Generated {num_segments_in_file} segments.\")\n",
    "\n",
    "                # Get the participant ID from filename to find the label\n",
    "                basename = os.path.basename(input_path)\n",
    "                participant_id = basename.split('_')[0] # Assumes 'sub-XXXXXX_...' format\n",
    "\n",
    "                if participant_id in label_map:\n",
    "                    age_label = label_map[participant_id]\n",
    "                    X_list.append(data_processed_segments)\n",
    "                    # Repeat the label for each segment generated from this file\n",
    "                    y_list.extend([age_label] * num_segments_in_file)\n",
    "                else:\n",
    "                    print(f\"    WARNING: Could not find label for participant {participant_id}. Skipping segments from this file.\")\n",
    "            # else:\n",
    "            #     print(f\"    WARNING: No segments generated after processing for {os.path.basename(input_path)}.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"    ERROR: File not found {input_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ERROR processing file {os.path.basename(input_path)}: {e}\")\n",
    "            # Consider adding more specific error handling if needed\n",
    "\n",
    "    # --- Final Assembly ---\n",
    "    if not X_list:\n",
    "        print(\"Error: No data segments were successfully processed and collected.\")\n",
    "        # Determine expected shape even if empty\n",
    "        final_n_channels = n_channels if n_channels is not None else 0\n",
    "        final_seg_len = segment_length_samples if segment_length_samples is not None else 0\n",
    "        return np.empty((0, final_n_channels, final_seg_len)), np.empty((0,))\n",
    "\n",
    "    # Concatenate all segments from all files\n",
    "    try:\n",
    "        X = np.concatenate(X_list, axis=0)\n",
    "        y = np.array(y_list)\n",
    "    except ValueError as e:\n",
    "         print(f\"Error concatenating data segments: {e}\")\n",
    "         # Print shapes of lists items for debugging\n",
    "         #for idx, item in enumerate(X_list):\n",
    "         #    print(f\" Shape of X_list[{idx}]: {item.shape}\")\n",
    "         return None, None # Indicate failure\n",
    "\n",
    "\n",
    "    print(f\"Finished loading. Final shapes: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "    # --- Sanity Check ---\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "         print(f\"CRITICAL WARNING: Mismatch between number of segments ({X.shape[0]}) and number of labels ({y.shape[0]})!\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:27:39.696795Z",
     "iopub.status.busy": "2025-05-26T02:27:39.696516Z",
     "iopub.status.idle": "2025-05-26T02:30:03.068811Z",
     "shell.execute_reply": "2025-05-26T02:30:03.066551Z",
     "shell.execute_reply.started": "2025-05-26T02:27:39.696771Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_137/3751606109.py:56: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  label_df = pd.read_csv(label_path, sep='\\\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data loading from: /kaggle/input/eeg-brain/Neural-Science/data\n",
      "Using segment length: 2.5 seconds\n",
      "Label file loaded. Columns: ['participant_id', 'age']\n",
      "Found 160 unique participant IDs with labels.\n",
      "Found 214 files matching labeled participants.\n",
      "Processing 214 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Files:   0%|          | 0/214 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Detected fs=250.00 Hz. Using segment length=625 samples (2.5s).\n",
      "    Setting expected number of channels to 52.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Files: 100%|██████████| 214/214 [02:11<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading. Final shapes: X=(48714, 52, 625), y=(48714,)\n",
      "\\n--- Data Loading Successful ---\n",
      "Loaded X shape: (48714, 52, 625)\n",
      "Loaded y shape: (48714,)\n"
     ]
    }
   ],
   "source": [
    "# Define the path and desired segment length\n",
    "data_path = '/kaggle/input/eeg-brain/Neural-Science/data'\n",
    "# <<< CHOOSE YOUR SEGMENT LENGTH HERE >>>\n",
    "segment_duration_seconds = 2.5 # Example: Use 5-second segments\n",
    "\n",
    "X, y = load_data_corrected(data_path, segment_length_seconds=segment_duration_seconds)\n",
    "\n",
    "# Check if loading was successful before proceeding\n",
    "if X is None or y is None or X.size == 0 or y.size == 0:\n",
    "    print(\"\\\\n--- Data Loading Failed. Stopping Execution. ---\")\n",
    "    # You might want to raise an error or handle this appropriately\n",
    "    # For now, just stop the notebook cell execution if in an interactive environment\n",
    "    assert False, \"Data loading failed, cannot continue.\"\n",
    "else:\n",
    "    print(\"\\\\n--- Data Loading Successful ---\")\n",
    "    print(f\"Loaded X shape: {X.shape}\") # Should be (total_segments, num_channels, segment_len_samples)\n",
    "    print(f\"Loaded y shape: {y.shape}\") # Should be (total_segments,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:30:03.070607Z",
     "iopub.status.busy": "2025-05-26T02:30:03.070326Z",
     "iopub.status.idle": "2025-05-26T02:31:09.578134Z",
     "shell.execute_reply": "2025-05-26T02:31:09.577272Z",
     "shell.execute_reply.started": "2025-05-26T02:30:03.070586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "# Assume you have functions to extract features like PSD\n",
    "# def extract_psd_features(segment_channel_data, fs=250, bands=None):\n",
    "#     # ... returns a 1D array of band powers for that single channel's segment data\n",
    "#     # e.g., [delta_power, theta_power, alpha_power, beta_power, gamma_power]\n",
    "#     if bands is None:\n",
    "#         bands = {'delta': [0.5, 4], 'theta': [4, 8], 'alpha': [8, 13], 'beta': [13, 30], 'gamma': [30, 50]}\n",
    "#     # Placeholder: Replace with actual PSD calculation (e.g., using Welch)\n",
    "#     # For simplicity, let's say it returns 5 features\n",
    "#     return np.random.rand(len(bands))\n",
    "\n",
    "\n",
    "# Assume X_train_tensor, y_train_tensor are your PyTorch tensors\n",
    "# X_train_tensor shape: (num_segments, num_channels, segment_length)\n",
    "# y_train_tensor shape: (num_segments, 1) or (num_segments,)\n",
    "\n",
    "# Convert to NumPy for scikit-learn\n",
    "X_train_np = X\n",
    "y_train_np = y.ravel() # Ensure y is 1D\n",
    "\n",
    "num_segments, num_channels, segment_length = X_train_np.shape\n",
    "#num_psd_bands = 5 # Example\n",
    "\n",
    "# 1. & 2. Extract features and flatten\n",
    "X_train_for_lasso = []\n",
    "for i in range(num_segments):\n",
    "    segment_features_flat = []\n",
    "    for j in range(num_channels):\n",
    "        channel_segment_data = X_train_np[i, j, :]\n",
    "        # channel_psd = extract_psd_features(channel_segment_data, fs=YOUR_SAMPLING_RATE)\n",
    "        # For demonstration, let's use mean and std as features per channel for simplicity\n",
    "        channel_mean = np.mean(channel_segment_data)\n",
    "        channel_std = np.std(channel_segment_data)\n",
    "        segment_features_flat.extend([channel_mean, channel_std]) # 3 features per channel\n",
    "    X_train_for_lasso.append(segment_features_flat)\n",
    "\n",
    "X_train_for_lasso = np.array(X_train_for_lasso)\n",
    "# Shape: (num_segments, num_channels * 2) in this simplified example\n",
    "\n",
    "# Scale features for Lasso\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_for_lasso = scaler.fit_transform(X_train_for_lasso)\n",
    "\n",
    "# 3. Apply Lasso\n",
    "# Alpha is the regularization strength. You'll need to tune this (e.g., via cross-validation)\n",
    "# Higher alpha = more sparsity (more zero coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:31:09.579472Z",
     "iopub.status.busy": "2025-05-26T02:31:09.579129Z",
     "iopub.status.idle": "2025-05-26T02:31:09.585475Z",
     "shell.execute_reply": "2025-05-26T02:31:09.584756Z",
     "shell.execute_reply.started": "2025-05-26T02:31:09.579448Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.linear_model import LassoCV\\n# ... (prepare X_train_scaled_for_lasso, y_train_np) ...\\nlasso_cv = LassoCV(cv=10, random_state=0, max_iter=5000) # cv=5 for 5-fold CV\\nlasso_cv.fit(X_train_scaled_for_lasso, y_train_np)\\nprint(f\"Optimal alpha found by LassoCV: {lasso_cv.alpha_}\")\\n# Use lasso_cv.coef_ for channel importance\\nlasso_coefs = lasso_cv.coef_\\n# Calculate importance per channel (example: sum of absolute coefficients)\\nnum_features_per_channel = 2 # In our simplified example (mean, std)\\nchannel_importance = []\\nfor j in range(num_channels):\\n    start_idx = j * num_features_per_channel\\n    end_idx = start_idx + num_features_per_channel\\n    channel_coefs = lasso_coefs[start_idx:end_idx]\\n    importance = np.sum(np.abs(channel_coefs))\\n    channel_importance.append(importance)\\n\\n# Select top N channels\\nchannel_indices_sorted_by_importance = np.argsort(channel_importance)[::-1] # Descending'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.linear_model import LassoCV\n",
    "# ... (prepare X_train_scaled_for_lasso, y_train_np) ...\n",
    "lasso_cv = LassoCV(cv=10, random_state=0, max_iter=5000) # cv=5 for 5-fold CV\n",
    "lasso_cv.fit(X_train_scaled_for_lasso, y_train_np)\n",
    "print(f\"Optimal alpha found by LassoCV: {lasso_cv.alpha_}\")\n",
    "# Use lasso_cv.coef_ for channel importance\n",
    "lasso_coefs = lasso_cv.coef_\n",
    "# Calculate importance per channel (example: sum of absolute coefficients)\n",
    "num_features_per_channel = 2 # In our simplified example (mean, std)\n",
    "channel_importance = []\n",
    "for j in range(num_channels):\n",
    "    start_idx = j * num_features_per_channel\n",
    "    end_idx = start_idx + num_features_per_channel\n",
    "    channel_coefs = lasso_coefs[start_idx:end_idx]\n",
    "    importance = np.sum(np.abs(channel_coefs))\n",
    "    channel_importance.append(importance)\n",
    "\n",
    "# Select top N channels\n",
    "channel_indices_sorted_by_importance = np.argsort(channel_importance)[::-1] # Descending'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:31:09.587128Z",
     "iopub.status.busy": "2025-05-26T02:31:09.586364Z",
     "iopub.status.idle": "2025-05-26T02:31:09.604560Z",
     "shell.execute_reply": "2025-05-26T02:31:09.603879Z",
     "shell.execute_reply.started": "2025-05-26T02:31:09.587104Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channel_importance_np = np.array(channel_importance)\\n\\n# --- MODIFIED CHANNEL SELECTION ---\\n# Select channels where the calculated importance is greater than zero\\nselected_channel_indices = np.where(channel_importance_np > 0)[0]\\nselected_channel_indices'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''channel_importance_np = np.array(channel_importance)\n",
    "\n",
    "# --- MODIFIED CHANNEL SELECTION ---\n",
    "# Select channels where the calculated importance is greater than zero\n",
    "selected_channel_indices = np.where(channel_importance_np > 0)[0]\n",
    "selected_channel_indices'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:31:09.608234Z",
     "iopub.status.busy": "2025-05-26T02:31:09.607818Z",
     "iopub.status.idle": "2025-05-26T02:31:10.656425Z",
     "shell.execute_reply": "2025-05-26T02:31:10.655799Z",
     "shell.execute_reply.started": "2025-05-26T02:31:09.608208Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected channel indices: [0, 1, 2, 8, 13, 20, 22, 27, 33, 34, 35, 44]\n"
     ]
    }
   ],
   "source": [
    "#NUM_CHANNELS_TO_SELECT = 20 # Example\n",
    "#selected_channel_indices = channel_indices_sorted_by_importance[:NUM_CHANNELS_TO_SELECT]\n",
    "selected_channel_indices = [ 0, 1, 2, 8, 13, 20, 22, 27, 33, 34, 35, 44]\n",
    "print(f\"Selected channel indices: {selected_channel_indices}\")\n",
    "X_selected_channels = X[:, selected_channel_indices, :]\n",
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0  1  2  8 13 20 22 27 33 34 35 44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:31:10.657349Z",
     "iopub.status.busy": "2025-05-26T02:31:10.657135Z",
     "iopub.status.idle": "2025-05-26T02:31:10.662579Z",
     "shell.execute_reply": "2025-05-26T02:31:10.661983Z",
     "shell.execute_reply.started": "2025-05-26T02:31:10.657332Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 8, 13, 20, 22, 27, 33, 34, 35, 44]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_channel_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:31:10.664071Z",
     "iopub.status.busy": "2025-05-26T02:31:10.663340Z",
     "iopub.status.idle": "2025-05-26T02:31:11.380418Z",
     "shell.execute_reply": "2025-05-26T02:31:11.379708Z",
     "shell.execute_reply.started": "2025-05-26T02:31:10.664045Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Tensors Created ---\n",
      "X_tensor shape: torch.Size([48714, 12, 625])\n",
      "y_tensor shape: torch.Size([48714, 1])\n"
     ]
    }
   ],
   "source": [
    "# Ensure data is valid before converting\n",
    "if X_selected_channels is not None and y is not None and X_selected_channels.size > 0 and y.size > 0:\n",
    "    X_tensor = torch.tensor(X_selected_channels, dtype=torch.float32) # Use float32 for PyTorch\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32) # Use float32 for regression\n",
    "\n",
    "    # Reshape y to match PyTorch requirements for loss function (batch_size, 1)\n",
    "    y_tensor = y_tensor.view(-1, 1)\n",
    "\n",
    "    print(f\"\\\\n--- Tensors Created ---\")\n",
    "    print(f\"X_tensor shape: {X_tensor.shape}\")\n",
    "    print(f\"y_tensor shape: {y_tensor.shape}\")\n",
    "else:\n",
    "    print(\"\\\\n--- Skipping Tensor Conversion due to Loading Errors ---\")\n",
    "    X_tensor, y_tensor = None, None # Ensure they are None if loading failed\n",
    "del X_selected_channels\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:31:11.381647Z",
     "iopub.status.busy": "2025-05-26T02:31:11.381424Z",
     "iopub.status.idle": "2025-05-26T02:31:12.272092Z",
     "shell.execute_reply": "2025-05-26T02:31:12.271492Z",
     "shell.execute_reply.started": "2025-05-26T02:31:11.381629Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- DataLoaders Created ---\n",
      "Train samples: 34100, Test samples: 14614\n",
      "Train batches: 2131, Test batches: 914\n"
     ]
    }
   ],
   "source": [
    "# Check if tensors are valid before splitting\n",
    "if X_tensor is not None and y_tensor is not None:\n",
    "    # !!! IMPORTANT NOTE ON SPLITTING !!!\n",
    "    # This basic train_test_split shuffles segments randomly.\n",
    "    # Segments from the SAME SUBJECT might end up in BOTH train and test sets.\n",
    "    # This leads to data leakage and overly optimistic results.\n",
    "    # A better approach is SUBJECT-AWARE splitting: Split participant IDs first,\n",
    "    # then collect all segments belonging to train/test subjects.\n",
    "    # For now, we proceed with the simple split as in the original code.\n",
    "\n",
    "    test_fraction = 0.3 # Use a fraction for robustness if total segments vary\n",
    "    test_size_count = int(len(X_tensor) * test_fraction)\n",
    "    # Ensure test_size is at least 1 if dataset is very small, adjust if needed\n",
    "    test_size_count = max(1, test_size_count) if len(X_tensor) > 0 else 0\n",
    "\n",
    "    if test_size_count > 0 and test_size_count < len(X_tensor):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_tensor, y_tensor, test_size=test_size_count, random_state=42, shuffle=True\n",
    "        )\n",
    "\n",
    "        # Create DataLoader for batch processing\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "        batch_size = 16 # Adjusted batch size, tune as needed\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True) # drop_last can help with stability\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        print(\"\\\\n--- DataLoaders Created ---\")\n",
    "        print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "        print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n",
    "    else:\n",
    "        print(\"\\\\n--- Not enough data to perform train/test split ---\")\n",
    "        train_loader, test_loader = None, None\n",
    "else:\n",
    "    print(\"\\\\n--- Skipping Train/Test Split due to Loading Errors ---\")\n",
    "    train_loader, test_loader = None, None\n",
    "#del X_tensor\n",
    "#del y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:31:12.272760Z",
     "iopub.status.busy": "2025-05-26T02:31:12.272563Z",
     "iopub.status.idle": "2025-05-26T02:31:12.328560Z",
     "shell.execute_reply": "2025-05-26T02:31:12.327693Z",
     "shell.execute_reply.started": "2025-05-26T02:31:12.272743Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE (predicting mean age): 15.292816162109375\n"
     ]
    }
   ],
   "source": [
    "mean_age_train = torch.mean(y_train)\n",
    "baseline_predictions = torch.full_like(y_test, mean_age_train) # y_val_numpy are true ages in validation\n",
    "baseline_mae = torch.mean(abs(y_test - baseline_predictions))\n",
    "print(f\"Baseline MAE (predicting mean age): {baseline_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:31:12.329783Z",
     "iopub.status.busy": "2025-05-26T02:31:12.329530Z",
     "iopub.status.idle": "2025-05-26T02:31:12.339358Z",
     "shell.execute_reply": "2025-05-26T02:31:12.338655Z",
     "shell.execute_reply.started": "2025-05-26T02:31:12.329755Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AttentionLSTMModel(nn.Module):\n",
    "    def __init__(self, num_channels, embedding_dim=128, lstm_hidden=64, nhead=1, dim_ff=256, dropout=0.3):\n",
    "        super(AttentionLSTMModel, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_proj = nn.Linear(num_channels, self.embedding_dim) # Project channels to embedding dim\n",
    "        self.pos_encoder = nn.Identity() # Placeholder: Consider adding positional encoding for transformer\n",
    "\n",
    "        # Attention Block (Transformer Encoder Layer)\n",
    "        # Ensure embedding_dim is divisible by nhead\n",
    "        if embedding_dim % nhead != 0:\n",
    "            # Find nearest valid nhead\n",
    "            valid_nheads = [h for h in range(1, embedding_dim + 1) if embedding_dim % h == 0]\n",
    "            nhead = valid_nheads[-1] if valid_nheads else 1 # Default to 1 if no divisors found\n",
    "            print(f\"Warning: embedding_dim ({embedding_dim}) not divisible by nhead (8). Adjusted nhead to {nhead}.\")\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embedding_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout, # Apply dropout within transformer\n",
    "            batch_first=True,\n",
    "            activation='relu' # Common activation\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2) # Single layer transformer\n",
    "\n",
    "        # LSTM Layers - Using nn.LSTM is generally preferred over nn.RNN\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=lstm_hidden, num_layers=1, bidirectional = True,batch_first=True, dropout=dropout if 1 > 1 else 0) # Dropout applies between layers if num_layers > 1\n",
    "        # A second LSTM layer might be too much, consider removing or reducing hidden size\n",
    "        # self.lstm2 = nn.LSTM(input_size=lstm_hidden, hidden_size=lstm_hidden // 2, num_layers=1, batch_first=True, dropout=dropout if 1 > 1 else 0)\n",
    "\n",
    "        # Regression Head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(lstm_hidden * 2), # Add LayerNorm before activation\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout), # Use consistent dropout rate\n",
    "            nn.Linear(lstm_hidden * 2, lstm_hidden // 2), # Add another layer\n",
    "            nn.LayerNorm(lstm_hidden // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_hidden // 2, 1) # Final output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels, timesteps)\n",
    "        # print(\"Input:\", x.shape)\n",
    "        x = x.permute(0, 2, 1)  # → (batch_size, timesteps, channels)\n",
    "        # print(\"Permuted:\", x.shape)\n",
    "        x = self.input_proj(x)  # → (batch_size, timesteps, embedding_dim)\n",
    "        # print(\"Projected:\", x.shape)\n",
    "\n",
    "        # Add positional encoding if using it\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        # Apply Transformer Encoder\n",
    "        x = self.transformer_encoder(x) # → (batch_size, timesteps, embedding_dim)\n",
    "        # print(\"After Transformer:\", x.shape)\n",
    "\n",
    "        # Apply LSTM\n",
    "        # LSTM output: output_seq, (h_n, c_n)\n",
    "        # output_seq shape: (batch_size, timesteps, hidden_size)\n",
    "        # h_n shape: (num_layers, batch_size, hidden_size)\n",
    "        lstm_out, (h_n, _) = self.lstm(x)\n",
    "        #print(\"After LSTM1 output:\", lstm_out.shape)\n",
    "        #print(\"After LSTM1 hidden:\", h_n.shape)\n",
    "\n",
    "        # Optional second LSTM\n",
    "        # lstm_out, (h_n, _) = self.lstm2(lstm_out)\n",
    "        # print(\"After LSTM2 output:\", lstm_out.shape)\n",
    "        # print(\"After LSTM2 hidden:\", h_n.shape)\n",
    "\n",
    "\n",
    "        # Get the output of the last time step from the last LSTM layer\n",
    "        # h_n is shape (num_layers, batch_size, hidden_size), we want the last layer's hidden state\n",
    "        #last_hidden_state = h_n[-1, :, :] # → (batch_size, hidden_size)\n",
    "        # Concatenate them along the feature dimension\n",
    "        last_hidden_state_forward = h_n[0, :, :]  # Shape: (batch_size, hidden_size)\n",
    "        last_hidden_state_backward = h_n[1, :, :] # Shape: (batch_size, hidden_size)\n",
    "\n",
    "        # Concatenate:\n",
    "        last_hidden_state = torch.cat((last_hidden_state_forward, last_hidden_state_backward), dim=1)\n",
    "        #print(\"Last Hidden State:\", last_hidden_state.shape)\n",
    "\n",
    "        # Alternatively, use the last time step of the output sequence:\n",
    "        # last_time_step_output = lstm_out[:, -1, :] # -> (batch_size, hidden_size)\n",
    "\n",
    "        x = self.fc(last_hidden_state) # Feed the final hidden state to the classifier\n",
    "        # print(\"Final Output:\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:31:12.340602Z",
     "iopub.status.busy": "2025-05-26T02:31:12.340271Z",
     "iopub.status.idle": "2025-05-26T02:31:15.132037Z",
     "shell.execute_reply": "2025-05-26T02:31:15.131184Z",
     "shell.execute_reply.started": "2025-05-26T02:31:12.340581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\\n--- Model Initialized ---\n",
      "AttentionLSTMModel(\n",
      "  (input_proj): Linear(in_features=12, out_features=128, bias=True)\n",
      "  (pos_encoder): Identity()\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.15, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.15, inplace=False)\n",
      "        (dropout2): Dropout(p=0.15, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
      "  (fc): Sequential(\n",
      "    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.15, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.15, inplace=False)\n",
      "    (7): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Total Trainable Parameters: 679,553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup\n",
    "# Ensure data loaders are valid before initializing model\n",
    "if train_loader is not None:\n",
    "    # Get input shape from the loaded data\n",
    "    # X_tensor shape: (total_segments, num_channels, segment_len_samples)\n",
    "    input_channels = 12\n",
    "    #X_tensor.shape[1] # Timesteps not directly needed by this model init\n",
    "\n",
    "    # --- Hyperparameters ---\n",
    "    EMBEDDING_DIM = 128 # Dimension after initial projection\n",
    "    LSTM_HIDDEN = 128   # LSTM hidden units\n",
    "    NHEAD = 2         # Transformer heads (must divide EMBEDDING_DIM)\n",
    "    DIM_FF = 512       # Transformer feedforward dim\n",
    "    DROPOUT = 0.15      # Dropout rate\n",
    "    LEARNING_RATE = 1e-4 # Learning rate\n",
    "    WEIGHT_DECAY = 1e-5  # Weight decay for regularization\n",
    "\n",
    "    model = AttentionLSTMModel(\n",
    "        num_channels=input_channels,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        lstm_hidden=LSTM_HIDDEN,\n",
    "        nhead=NHEAD,\n",
    "        dim_ff=DIM_FF,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    criterion = nn.L1Loss() # MAE Loss for age regression\n",
    "    #criterion = nn.MSELoss() # Option: Mean Squared Error\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY) # AdamW is often preferred\n",
    "    num_training_steps = 100 * len(train_loader)\n",
    "    num_warmup_steps = 100\n",
    "    #int(0.01 * num_training_steps)  # 10% warmup\n",
    "\n",
    "    '''scheduler = get_polynomial_decay_schedule_with_warmup(\n",
    "                optimizer, \n",
    "                num_warmup_steps=num_warmup_steps,\n",
    "                num_training_steps=num_training_steps\n",
    "    )'''\n",
    "    '''get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    "    )'''\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5, verbose=True, min_lr=1e-6) # More patience, lower factor\n",
    "\n",
    "    # Print model summary\n",
    "    print(\"\\\\n--- Model Initialized ---\")\n",
    "    print(model)\n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Trainable Parameters: {total_params:,}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\\\n--- Skipping Model Initialization due to Loading/Splitting Errors ---\")\n",
    "    model = None\n",
    "    criterion = None\n",
    "    optimizer = None\n",
    "    scheduler = None\n",
    "    device = \"cpu\" # Default to CPU if no model/GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:31:15.133380Z",
     "iopub.status.busy": "2025-05-26T02:31:15.132795Z",
     "iopub.status.idle": "2025-05-26T02:31:15.142107Z",
     "shell.execute_reply": "2025-05-26T02:31:15.141483Z",
     "shell.execute_reply.started": "2025-05-26T02:31:15.133353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, path_to_save, patience=10, delta=0.001, restore_best_weights=True, verbose=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path_to_save (str): Path to save the best model state.\n",
    "            patience (int): How many epochs to wait after last time validation loss improved.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            restore_best_weights (bool): Whether to restore model weights from the epoch with the best value of the monitored quantity.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.path_to_save = path_to_save\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_val_loss = np.Inf\n",
    "        self.best_epoch = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "\n",
    "    def __call__(self, val_loss, epoch, model):\n",
    "        score = -val_loss # We want to maximize the negative loss (minimize loss)\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_val_loss = val_loss\n",
    "            self.best_epoch = epoch\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                 print(f'EarlyStopping counter: {self.counter} out of {self.patience} (Best val_loss: {self.best_val_loss:.4f} at epoch {self.best_epoch})')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "        else:\n",
    "            # Improvement detected\n",
    "            self.best_score = score\n",
    "            self.best_val_loss = val_loss\n",
    "            self.best_epoch = epoch\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0 # Reset counter\n",
    "\n",
    "        return self.early_stop\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_val_loss:.4f} --> {val_loss:.4f}). Saving model to {self.path_to_save} ...')\n",
    "        # Save the current best weights\n",
    "        self.best_weights = model.state_dict()\n",
    "        torch.save(self.best_weights, self.path_to_save)\n",
    "        # Update the stored best loss (needed if comparing score < best_score + delta)\n",
    "        # self.best_val_loss = val_loss # Already updated before calling save_checkpoint\n",
    "\n",
    "    def load_best_weights(self, model):\n",
    "         if self.restore_best_weights and self.best_weights is not None:\n",
    "             print(f\"Restoring model weights from epoch {self.best_epoch} with val_loss: {self.best_val_loss:.4f}\")\n",
    "             model.load_state_dict(self.best_weights)\n",
    "         elif self.restore_best_weights:\n",
    "              print(\"Warning: restore_best_weights=True but no best weights were saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:31:15.143256Z",
     "iopub.status.busy": "2025-05-26T02:31:15.142898Z",
     "iopub.status.idle": "2025-05-26T02:31:15.166187Z",
     "shell.execute_reply": "2025-05-26T02:31:15.165477Z",
     "shell.execute_reply.started": "2025-05-26T02:31:15.143228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval_model(model, train_loader, test_loader, criterion, optimizer, device, epochs=100, scheduler=None, early_stopping=None):\n",
    "\n",
    "    if model is None or train_loader is None or test_loader is None:\n",
    "        print(\"Model or DataLoaders not initialized. Skipping training.\")\n",
    "        return [], []\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    print(\"\\\\n--- Starting Training ---\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        # Use tqdm for progress bar\n",
    "        train_pbar = tqdm(train_loader, unit='batch', desc=f'Train Epoch {epoch + 1}/{epochs}')\n",
    "        for X_batch, y_batch in train_pbar:\n",
    "            # Move data to the correct device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            ypred = model(X_batch)\n",
    "            loss = criterion(ypred, y_batch)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Optional: Gradient Clipping\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item() * X_batch.size(0) # Weighted average contribution\n",
    "            train_pbar.set_postfix(loss=loss.item()) # Show loss in progress bar\n",
    "\n",
    "\n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        print(f'---> Epoch {epoch + 1} Train Loss: {epoch_train_loss:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_test_loss = 0.0\n",
    "        test_pbar = tqdm(test_loader, unit='batch', desc=f'Eval Epoch {epoch + 1}/{epochs}', leave=False)\n",
    "        with torch.no_grad(): # Use no_grad for evaluation efficiency\n",
    "            for X_batch, y_batch in test_pbar:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                ypred = model(X_batch)\n",
    "                loss = criterion(ypred, y_batch)\n",
    "                running_test_loss += loss.item() * X_batch.size(0)\n",
    "                test_pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_test_loss = running_test_loss / len(test_loader.dataset)\n",
    "        test_losses.append(epoch_test_loss)\n",
    "        print(f'---> Epoch {epoch + 1} Test Loss:  {epoch_test_loss:.4f}')\n",
    "\n",
    "        # Learning rate scheduling step\n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(epoch_test_loss) # Pass validation loss\n",
    "                # Print current LR (optional)\n",
    "                # current_lr = optimizer.param_groups[0]['lr']\n",
    "                # print(f\"    Current LR: {current_lr:.6f}\")\n",
    "            else:\n",
    "                 scheduler.step() # For other schedulers like StepLR\n",
    "\n",
    "        # Early stopping check\n",
    "        if early_stopping:\n",
    "            if early_stopping(epoch_test_loss, epoch + 1, model): # Pass epoch number (1-based)\n",
    "                break # Stop training loop\n",
    "\n",
    "        print(\"-\" * 50) # Separator between epochs\n",
    "\n",
    "    print(\"--- Training Finished ---\")\n",
    "\n",
    "    # Load best weights if early stopping was used and configured to restore\n",
    "    if early_stopping:\n",
    "        early_stopping.load_best_weights(model)\n",
    "\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T02:31:15.167196Z",
     "iopub.status.busy": "2025-05-26T02:31:15.166953Z",
     "iopub.status.idle": "2025-05-26T03:03:26.442402Z",
     "shell.execute_reply": "2025-05-26T03:03:26.441647Z",
     "shell.execute_reply.started": "2025-05-26T02:31:15.167176Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/10: 100%|██████████| 2131/2131 [03:02<00:00, 11.65batch/s, loss=36.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 1 Train Loss: 42.6945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 1 Test Loss:  36.0148\n",
      "Validation loss decreased (36.0148 --> 36.0148). Saving model to best_model_cnnlstm_corrected.pth ...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2/10: 100%|██████████| 2131/2131 [03:02<00:00, 11.70batch/s, loss=24.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 2 Train Loss: 28.4062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 2 Test Loss:  21.7879\n",
      "Validation loss decreased (21.7879 --> 21.7879). Saving model to best_model_cnnlstm_corrected.pth ...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3/10: 100%|██████████| 2131/2131 [03:02<00:00, 11.70batch/s, loss=14.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 3 Train Loss: 18.0983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 3 Test Loss:  15.6682\n",
      "Validation loss decreased (15.6682 --> 15.6682). Saving model to best_model_cnnlstm_corrected.pth ...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4/10: 100%|██████████| 2131/2131 [03:02<00:00, 11.70batch/s, loss=15.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 4 Train Loss: 15.5991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 4 Test Loss:  15.2722\n",
      "Validation loss decreased (15.2722 --> 15.2722). Saving model to best_model_cnnlstm_corrected.pth ...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5/10: 100%|██████████| 2131/2131 [03:02<00:00, 11.70batch/s, loss=12.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 5 Train Loss: 15.4999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 5 Test Loss:  15.2825\n",
      "EarlyStopping counter: 1 out of 50 (Best val_loss: 15.2722 at epoch 4)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6/10: 100%|██████████| 2131/2131 [03:02<00:00, 11.70batch/s, loss=17.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 6 Train Loss: 15.4759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 6 Test Loss:  15.2734\n",
      "EarlyStopping counter: 2 out of 50 (Best val_loss: 15.2722 at epoch 4)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7/10: 100%|██████████| 2131/2131 [03:02<00:00, 11.70batch/s, loss=15.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 7 Train Loss: 15.4831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 7 Test Loss:  15.2780\n",
      "EarlyStopping counter: 3 out of 50 (Best val_loss: 15.2722 at epoch 4)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8/10: 100%|██████████| 2131/2131 [03:02<00:00, 11.69batch/s, loss=13.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 8 Train Loss: 15.4915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 8 Test Loss:  15.2797\n",
      "EarlyStopping counter: 4 out of 50 (Best val_loss: 15.2722 at epoch 4)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 9/10: 100%|██████████| 2131/2131 [03:02<00:00, 11.69batch/s, loss=14.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 9 Train Loss: 15.5460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 9 Test Loss:  15.2781\n",
      "EarlyStopping counter: 5 out of 50 (Best val_loss: 15.2722 at epoch 4)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 10/10: 100%|██████████| 2131/2131 [03:02<00:00, 11.69batch/s, loss=9.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 10 Train Loss: 15.4659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 10 Test Loss:  15.2772\n",
      "EarlyStopping counter: 6 out of 50 (Best val_loss: 15.2722 at epoch 4)\n",
      "--------------------------------------------------\n",
      "--- Training Finished ---\n",
      "Restoring model weights from epoch 4 with val_loss: 15.2722\n",
      "\\n--- Post-Training Evaluation ---\n",
      "Minimum Training Loss: 15.4659\n",
      "Minimum Validation Loss: 15.2722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Ensure model and dataloaders are ready\n",
    "if model and train_loader and test_loader and criterion and optimizer and device:\n",
    "    # Initialize Early Stopping\n",
    "    early_stopper = EarlyStopping(path_to_save='best_model_cnnlstm_corrected.pth', patience=50, delta=0.01, verbose=True) # Increased patience\n",
    "\n",
    "    # --- Start Training ---\n",
    "    num_epochs = 10 # Adjust as needed\n",
    "    train_losses, test_losses = train_and_eval_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device, # Pass the device\n",
    "        epochs=num_epochs,\n",
    "        scheduler=scheduler,\n",
    "        early_stopping=early_stopper\n",
    "    )\n",
    "\n",
    "    print(\"\\\\n--- Post-Training Evaluation ---\")\n",
    "    print(f\"Minimum Training Loss: {np.min(train_losses):.4f}\")\n",
    "    print(f\"Minimum Validation Loss: {np.min(test_losses):.4f}\")\n",
    "    if early_stopper.early_stop:\n",
    "         print(f\"Stopped early at epoch {early_stopper.best_epoch}. Best Validation Loss: {early_stopper.best_val_loss:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\\\n--- Cannot Start Training due to Initialization Errors ---\")\n",
    "    train_losses, test_losses = [], [] # Assign empty lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T03:03:26.443563Z",
     "iopub.status.busy": "2025-05-26T03:03:26.443261Z",
     "iopub.status.idle": "2025-05-26T03:03:26.688324Z",
     "shell.execute_reply": "2025-05-26T03:03:26.687473Z",
     "shell.execute_reply.started": "2025-05-26T03:03:26.443535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T03:03:26.689465Z",
     "iopub.status.busy": "2025-05-26T03:03:26.689164Z",
     "iopub.status.idle": "2025-05-26T03:03:26.693211Z",
     "shell.execute_reply": "2025-05-26T03:03:26.692473Z",
     "shell.execute_reply.started": "2025-05-26T03:03:26.689438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# if train_losses and test_losses:\n",
    "#     print(f\"Final Minimum Training Loss: {np.min(train_losses):.4f}\")\n",
    "#     print(f\"Final Minimum Validation Loss: {np.min(test_losses):.4f}\")\n",
    "# else:\n",
    "#      print(\"No training results available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T03:03:26.694309Z",
     "iopub.status.busy": "2025-05-26T03:03:26.694038Z",
     "iopub.status.idle": "2025-05-26T03:03:26.951669Z",
     "shell.execute_reply": "2025-05-26T03:03:26.950765Z",
     "shell.execute_reply.started": "2025-05-26T03:03:26.694284Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if train_losses and test_losses:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Validation Loss')\n",
    "\n",
    "    # Find the epoch with the minimum validation loss\n",
    "    min_val_loss_epoch = np.argmin(test_losses)\n",
    "    min_val_loss = np.min(test_losses)\n",
    "    plt.scatter(min_val_loss_epoch, min_val_loss, color='red', zorder=5, label=f'Best Val Loss: {min_val_loss:.4f}')\n",
    "\n",
    "    # If early stopping happened, mark where it stopped conceptually (best epoch)\n",
    "    if 'early_stopper' in globals() and early_stopper.early_stop:\n",
    "         plt.axvline(early_stopper.best_epoch -1 , color='r', linestyle='--', label=f'Best Epoch: {early_stopper.best_epoch}') # epoch is 1-based\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (L1 MAE)')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training data to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T03:03:26.952736Z",
     "iopub.status.busy": "2025-05-26T03:03:26.952499Z",
     "iopub.status.idle": "2025-05-26T03:03:27.648377Z",
     "shell.execute_reply": "2025-05-26T03:03:27.647564Z",
     "shell.execute_reply.started": "2025-05-26T03:03:26.952719Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "np.save(\"X_train_corrected.npy\", X_train.cpu().numpy() if 'X_train' in globals() else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T03:03:27.649407Z",
     "iopub.status.busy": "2025-05-26T03:03:27.649216Z",
     "iopub.status.idle": "2025-05-26T03:03:28.314533Z",
     "shell.execute_reply": "2025-05-26T03:03:28.313805Z",
     "shell.execute_reply.started": "2025-05-26T03:03:27.649392Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Info | 11 non-empty values\n",
      " bads: []\n",
      " ch_names: parcel_0, parcel_1, parcel_2, parcel_3, parcel_4, parcel_5, ...\n",
      " chs: 52 misc, 17 Stimulus\n",
      " custom_ref_applied: False\n",
      " description: Anonymized using a time shift to preserve age at acquisition OSL ...\n",
      " dig: 0 items\n",
      " file_id: 4 items (dict)\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 1916-01-09 10:21:05 UTC\n",
      " meas_id: 4 items (dict)\n",
      " nchan: 69\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "# Define the path to the file\n",
    "file_path = \"/kaggle/input/eeg-brain/Neural-Science/data/group_1/sub-CC110033_sflip_parc-raw.fif\"\n",
    "# Load the raw data\n",
    "raw = mne.io.read_raw_fif(file_path, preload=True)\n",
    "# Display information about the raw object\n",
    "print(raw.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T03:03:28.315549Z",
     "iopub.status.busy": "2025-05-26T03:03:28.315284Z",
     "iopub.status.idle": "2025-05-26T03:03:28.378786Z",
     "shell.execute_reply": "2025-05-26T03:03:28.377901Z",
     "shell.execute_reply.started": "2025-05-26T03:03:28.315522Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Info | 11 non-empty values\n",
      " bads: []\n",
      " ch_names: parcel_0, parcel_1, parcel_2, parcel_3, parcel_4, parcel_5, ...\n",
      " chs: 52 misc\n",
      " custom_ref_applied: False\n",
      " description: Anonymized using a time shift to preserve age at acquisition OSL ...\n",
      " dig: 0 items\n",
      " file_id: 4 items (dict)\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 1916-01-09 10:21:05 UTC\n",
      " meas_id: 4 items (dict)\n",
      " nchan: 52\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "# Extract only MISC channels\n",
    "misc_channels = raw.copy().pick_types(misc=True)\n",
    "# Display information about the extracted channels\n",
    "print(misc_channels.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T03:03:28.380144Z",
     "iopub.status.busy": "2025-05-26T03:03:28.379645Z",
     "iopub.status.idle": "2025-05-26T03:03:28.385215Z",
     "shell.execute_reply": "2025-05-26T03:03:28.384639Z",
     "shell.execute_reply.started": "2025-05-26T03:03:28.380118Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total recording time: 574.00 seconds\n"
     ]
    }
   ],
   "source": [
    "# Get the total recording time in seconds\n",
    "total_duration = raw.times[-1]\n",
    "# Alternatively, using the raw.info dictionary\n",
    "total_duration_alt = raw.n_times / raw.info['sfreq']\n",
    "# Print the total duration\n",
    "print(f\"Total recording time: {total_duration:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7153305,
     "sourceId": 11421984,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
